import os, sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from openai import OpenAI
import json
from dotenv import load_dotenv
load_dotenv()

client = OpenAI()

class OpenAITool:
    def __init__():
        pass

    def run_tools(tools):
        tool_metadata = [tool.args_schema for tool in tools]
        # Create a running input list we will add to over time
        input_list = [
            {"role": "user", "content": "What is the weather this month?"}
        ]

        # 2. Prompt the model with tools defined

        # First, call LLM to get the tool params
        response = client.responses.create(
            model="gpt-4.1",
            instructions="Generate relevant tool arguments using the list of tools",
            tools=tool_metadata,
            input=input_list,
        )

        # Save function call outputs for subsequent requests
        input_list += response.output

        for item in response.output:
            print("item: ", item)
            if item.type == "function_call":
                # 3. Execute the function logic
                for tool in tools:
                    if item.name == tool.name:
                        args = json.loads(item.arguments)
                        result = tool(**args)
                        print("result: ", result)
                
                # # 4. Provide function call results to the model
                # input_list.append({
                #     "type": "function_call_output",
                #     "call_id": item.call_id,
                #     "output": json.dumps({
                #     "result": result
                #     })
                # })

        # print("Final input:")
        # print(input_list)

        # # Second, call LLM to generate a response based on the tools called
        # response = client.responses.create(
        #     model="gpt-4.1",
        #     instructions="Respond only with the relevant answer generated by a tool.",
        #     tools=tools,
        #     input=input_list,
        # )

        # # 5. The model should be able to give a response!
        # print("Final output:")
        # print(response.model_dump_json(indent=2))
        # print("\n" + response.output_text)