import os, sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from openai import OpenAI
import json
from dotenv import load_dotenv
load_dotenv()
from utils.serializable import to_serializable

client = OpenAI()

class OpenAIToolCall:
    def __init__(self):
        pass

    def to_openai_tool(self, tool_dict: dict) -> dict:
        schema = tool_dict.get("args_schema")

        return {
            "type": "function",
            "name": schema.get("name", ""),
            "description": schema.get("description", tool_dict.get("description", "")),
            "parameters": schema.get("parameters", {"type": "object", "properties": {}}),
        }

    async def run_tools(self, tools, input_list):
        tool_metadata = [tool.args_schema for tool in tools]

        print("tool metadata: ", tool_metadata)

        # Call LLM to get the tool params
        response = client.responses.create(
            model="gpt-4.1",
            instructions="Generate relevant tool arguments using the list of tools",
            tools=tool_metadata,
            input=input_list,
        )

        # Save function call outputs for subsequent requests
        input_list += response.output

        for item in response.output:
            if item.type == "function_call":
                # 3. Execute the function logic
                for tool in tools:
                    if item.name == tool.name:
                        args = json.loads(item.arguments)
                        if tool.is_async is True:
                            result = await tool(**args)
                        else:
                            result = tool(**args)
                        print("result: ", result)
                
                # # 4. Provide function call results to the model
                input_list.append({
                    "type": "function_call_output",
                    "call_id": item.call_id,
                    "output": json.dumps({
                        f"{item.name}_result": result
                    })
                })

        serialized_tools = to_serializable(tools)
        tools = [self.to_openai_tool(t) for t in serialized_tools]

        # Call LLM to generate a response based on the tools called
        response = client.responses.create(
            model="gpt-4.1",
            instructions="Respond only with the relevant answer generated by a tool.",
            tools=tools,
            input=input_list,
        )

        # The model should be able to give a response!
        print("Final output:")
        print(response)
        print(response.model_dump_json(indent=2))
        print("\n" + response.output_text)